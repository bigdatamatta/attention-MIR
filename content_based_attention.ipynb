{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from numpy import exp, sqrt, dot\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import loadmat\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import moment\n",
    "from scipy.stats import skew\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data into bootstrapped sequences tf.Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"MODIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    if dataset == \"CORN\" or dataset == \"WHEAT\":\n",
    "        full_data = pickle.load(open(dataset + \".p\", \"rb\"))\n",
    "        columns = ([\"id\"] +  ['label'] + [\"reflectance_\" + str(i) for i in range(92)])\n",
    "        full_data.columns = columns\n",
    "        \n",
    "    else:\n",
    "        mat_dict = loadmat('../MIR/' + dataset + '.mat')\n",
    "        full_data = pd.DataFrame(mat_dict[dataset])\n",
    "\n",
    "        # Rename columns to something more interpretable\n",
    "        columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    "                   + [\"solar_\" + str(i) for i in range(5)] + ['label'])\n",
    "        full_data.columns = columns\n",
    "\n",
    "        if dataset == \"MISR2\":\n",
    "            full_data = full_data[full_data['id'].isin(list(full_data['id'].value_counts().index[full_data['id'].value_counts() == 100]))]\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moments_df(n_moments):\n",
    "    \n",
    "    full_data = load_data()\n",
    "\n",
    "#     non_constant_cols = ['id', 'reflectance_0', 'reflectance_1', 'reflectance_2',\n",
    "#            'reflectance_3', 'reflectance_4', 'reflectance_5', 'reflectance_6']\n",
    "    \n",
    "    non_constant_cols = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(12)])\n",
    "\n",
    "    constant_columns = ['solar_0', 'solar_1', 'solar_2', 'solar_3']\n",
    "    list_operation = ['np.mean', 'np.var', 'skew', 'kurtosis']\n",
    "\n",
    "    full_data_subset = full_data[non_constant_cols]\n",
    "    \n",
    "    dic_df = {}\n",
    "\n",
    "    for moments in range(1, n_moments+1):\n",
    "        if moments < 5:\n",
    "            dic_df[moments] = eval(\"np.stack(full_data_subset.groupby('id').apply(lambda group: \" + \n",
    "                                   list_operation[moments-1] + \"(group)).values)[:,1:]\")\n",
    "        else:\n",
    "            dic_df[moments] = np.stack(full_data_subset.groupby('id').apply(lambda group: moment(group, moment=moments)))[:,1:]\n",
    "\n",
    "    array_data = dic_df[1]\n",
    "    \n",
    "    if n_moments > 1:\n",
    "        for key in range(2, n_moments+1):\n",
    "            array_data = np.hstack([array_data, dic_df[key]])   \n",
    "\n",
    "    df = pd.DataFrame(array_data)\n",
    "    if dataset != \"CORN\" and dataset != \"WHEAT\":\n",
    "        df = pd.concat([df, full_data.groupby('id').mean()[constant_columns].reset_index()], axis=1)\n",
    "    df['id'] = full_data['id'].unique()\n",
    "    df['label'] = full_data.groupby('id').mean()['label'].values\n",
    "    df = df.set_index('id')\n",
    "    \n",
    "    col_label = ['label']\n",
    "    labels = df[col_label]\n",
    "    df = df[[col for col in df.columns if col not in col_label]]\n",
    "    \n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sequences(train, test):\n",
    "    list_sequence_train, list_sequence_test = [], []\n",
    "    list_train_y, list_test_y = [], []\n",
    "    features = [col for col in train.columns if col not in ['id', 'label']]\n",
    "    \n",
    "    for bag_id in list(train['id'].unique()):\n",
    "        list_sequence_train.append(np.array(train[train['id']==bag_id][features]))\n",
    "        list_train_y.append(np.array(train[train['id']==bag_id]['label'])[0])\n",
    "        \n",
    "    for bag_id in list(test['id'].unique()):\n",
    "        list_sequence_test.append(np.array(test[test['id']==bag_id][features]))\n",
    "        list_test_y.append(np.array(test[test['id']==bag_id]['label'])[0])\n",
    "    \n",
    "    return list_sequence_train, list_sequence_test, list_train_y, list_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_attention(random_seed, processor_steps, splits=5, use_moments=False, n_moments=1, concat_late=False):\n",
    "\n",
    "    full_data = load_data()\n",
    "    num_features = 12\n",
    "    count = 1\n",
    "    \n",
    "    if use_moments:\n",
    "        num_features = 92 + n_moments*92\n",
    "        df, _ = moments_df(n_moments)\n",
    "        col_solar = ['solar_0', 'solar_1', 'solar_2', 'solar_3', 'solar_4']\n",
    "        df = df.iloc[np.repeat(np.arange(len(df)), 100)]\n",
    "        df = df[[col for col in df.columns if col not in col_solar]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        full_data = pd.concat([full_data, df], axis=1)\n",
    "    \n",
    "    kf = KFold(n_splits=splits, shuffle=True, random_state=random_seed)\n",
    "    cols_exclude = [\"id\", \"label\"]\n",
    "    features = [col for col in list(full_data.columns) if col not in cols_exclude]\n",
    "    lowest_val_list, loss_test_list = [], []\n",
    "    \n",
    "    for train_index, test_index in kf.split(list(full_data['id'].unique())):\n",
    "        #print('Compute for FOLD NUMBER ' + str(count))\n",
    "        train_index = full_data['id'].unique()[train_index]\n",
    "        test_index = full_data['id'].unique()[test_index]\n",
    "        train = full_data[full_data['id'].apply(lambda value: value in train_index)]\n",
    "        test = full_data[full_data['id'].apply(lambda value: value in test_index)]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train[features])\n",
    "        train[features], test[features] = scaler.transform(train[features]), scaler.transform(test[features])\n",
    "    \n",
    "        list_sequence_train, list_sequence_test, train_y, test_y = make_sequences(train, test)\n",
    "        lowest_val_loss, loss_test = attention(list_sequence_train, list_sequence_test, train_y, test_y, \n",
    "                                               num_features, processor_steps, concat_late)\n",
    "        \n",
    "        lowest_val_list.append(lowest_val_loss)\n",
    "        loss_test_list.append(loss_test)\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        count += 1\n",
    "    \n",
    "    return lowest_val_loss, loss_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(pred_array, truth_array):\n",
    "    loss = np.sqrt(mean_squared_error(np.reshape(np.array(pred_array), (np.array(pred_array).shape[0],1)), \n",
    "                                               np.reshape(np.array(truth_array), (np.array(truth_array).shape[0],1))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the neighbours of the minimum to be sure it is not due to stochasticity\n",
    "def get_min_index(array, n_max=3):\n",
    "    indices = array.argsort()[:n_max] \n",
    "    \n",
    "    avg_neighbourgs = []\n",
    "    for val in indices:\n",
    "        if val == len(array)-1:\n",
    "            avg_neighbourgs.append((array[val]+array[val-1]+array[val-2])/3)\n",
    "        else:\n",
    "            avg_neighbourgs.append((array[val]+array[val-1]+array[val+1])/3)\n",
    "    \n",
    "    index = indices[np.argmin(avg_neighbourgs)]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network hyperparameters\n",
    "\n",
    "batch_size = 1\n",
    "sequence_lenght = 100\n",
    "dropout_p = 0.5\n",
    "\n",
    "# reading block Neural Network size\n",
    "size_h1 = 256\n",
    "\n",
    "n_epochs = 500\n",
    "num_test_runs = 1\n",
    "learning_rate = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(list_sequence_train, list_sequence_val, train_y, val_y, num_features, processor_steps, concat_late=False):\n",
    "    \n",
    "    with tf.variable_scope('data'):\n",
    "        \n",
    "        prob = tf.placeholder_with_default(1.0, shape=())\n",
    "        X_or = tf.placeholder(shape = [sequence_lenght, num_features], dtype = tf.float32, name = \"input\")\n",
    "        if concat_late:\n",
    "            X_or = tf.placeholder(shape = [sequence_lenght, 12], dtype = tf.float32, name = \"input\")\n",
    "            X_fixed = tf.placeholder(shape = [1, num_features-12], dtype = tf.float32, name = \"fixed_input\")\n",
    "            fixed_num_features = num_features-12\n",
    "            num_features = 12\n",
    "        y = tf.placeholder(shape = [1,1], dtype = tf.float32, name = \"label_annotation\")\n",
    "        \n",
    "    with tf.variable_scope('reading_block'):\n",
    "    \n",
    "        W_reading1 = tf.Variable(tf.random_normal(shape = (num_features, int(size_h1/2)), stddev = 0.1), name = \"weights_hidden1\", trainable = True)\n",
    "        hidden_relu1 = tf.matmul(X_or, W_reading1)\n",
    "        BN1 = tf.nn.relu(hidden_relu1, name = \"ReLu_hidden_layer1\")\n",
    "        BN1 = tf.nn.dropout(BN1, prob, noise_shape=[1,int(size_h1/2)])\n",
    "        \n",
    "    with tf.variable_scope('initialize_lstm'):\n",
    "        \n",
    "        static_input = tf.zeros(shape=[1,1,1])\n",
    "        unstack_input = tf.unstack(static_input, 1, 1)\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(size_h1, reuse=tf.AUTO_REUSE, state_is_tuple=True)\n",
    "        initial_state = lstm_cell.zero_state(1, dtype=tf.float32)\n",
    "        print(initial_state, initial_state[0], initial_state[-1])\n",
    "        \n",
    "    for index in range(processor_steps):\n",
    "        with tf.variable_scope('lstm_cell_decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            output_decoder, state_decoder = tf.nn.static_rnn(lstm_cell, inputs=unstack_input,\n",
    "                                                             initial_state=initial_state, dtype=tf.float32)\n",
    "            \n",
    "            print(output_decoder, state_decoder)\n",
    "            print(\"for hidden/memory state: \", state_decoder[-1])\n",
    "\n",
    "            W_decoder = tf.get_variable(initializer=tf.random_normal(shape = (size_h1, int(size_h1/2)), stddev = 0.1), name = \"decoder_weights\", trainable = True)\n",
    "\n",
    "            relu_decoder = tf.nn.relu(tf.matmul(state_decoder[-1], W_decoder))\n",
    "            relu_decoder = tf.nn.dropout(relu_decoder, prob, noise_shape=[1,int(size_h1/2)])\n",
    "\n",
    "            inner_product = tf.matmul(BN1, tf.reshape(relu_decoder, shape=(int(size_h1/2), 1)))\n",
    "            reshaped_inner_product = tf.reshape(inner_product, shape=(1, sequence_lenght))\n",
    "#             W_attention = tf.get_variable(initializer=tf.random_normal(shape = (sequence_lenght, sequence_lenght), \n",
    "#                                                                        stddev = 0.1), name = \"w_attention\", trainable = True)\n",
    "#             reshaped_inner_product = tf.nn.sigmoid(tf.matmul(reshaped_inner_product, W_attention))\n",
    "            \n",
    "            attention_score =  tf.nn.softmax(reshaped_inner_product)\n",
    "            context_vector = tf.multiply(BN1, tf.reshape(attention_score, (sequence_lenght, 1)))\n",
    "            r_vector = tf.reshape(tf.reduce_sum(context_vector, axis=0), (1,int(size_h1/2)))\n",
    "            q_star = tf.concat([relu_decoder, r_vector], axis=1)\n",
    "\n",
    "            initial_state = (initial_state[0], q_star)\n",
    "            \n",
    "    with tf.variable_scope('logits'):\n",
    "        if concat_late:\n",
    "            q_star_concat = tf.concat([q_star, X_fixed], axis=1)\n",
    "            q_star_concat = tf.nn.dropout(q_star_concat, prob)\n",
    "            W_concat = tf.Variable(tf.random_normal(shape = (size_h1+fixed_num_features, 1), stddev = 0.1), name = \"weights_concat\", trainable = True)\n",
    "            logits = tf.nn.relu(tf.matmul(q_star_concat, W_concat))\n",
    "        else:   \n",
    "            W_logits = tf.Variable(tf.random_normal(shape = (size_h1, 1), stddev = 0.1), name = \"weights_logits\", trainable = True)\n",
    "            logits = tf.nn.relu(tf.matmul(q_star, W_logits))\n",
    "        \n",
    "    with tf.variable_scope('loss'):\n",
    "        \n",
    "        loss =  tf.losses.mean_squared_error(predictions = logits, labels = y)\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    print(tf.trainable_variables())\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    val_loss_list, test_loss_list = [], []\n",
    "\n",
    "    for i in range(1, n_epochs+1):\n",
    "    \n",
    "        train_pred, train_truth = [], []\n",
    "        val_test_pred, val_test_truth = [], []\n",
    "        indices = list(range(len(list_sequence_train)))\n",
    "        shuffle(indices)\n",
    "\n",
    "        for index in indices:\n",
    "            # batch is size 1 here: stochastic gradient descent\n",
    "            if concat_late:\n",
    "                X_batch_or = np.reshape(list_sequence_train[index][:,:12], (100, num_features))\n",
    "                X_batch_fixed = np.reshape(list_sequence_train[index][0,12:], (1, fixed_num_features))\n",
    "                Y_batch = np.reshape(train_y[index], (1,1))\n",
    "                _, pred = sess.run([train_op, logits], feed_dict = {X_or: X_batch_or, X_fixed: X_batch_fixed,\n",
    "                                                                    y: Y_batch, prob: dropout_p})\n",
    "            else:\n",
    "                X_batch = np.reshape(list_sequence_train[index], (100, num_features))\n",
    "                Y_batch = np.reshape(train_y[index], (1,1))\n",
    "                _, pred = sess.run([train_op, logits], feed_dict = {X_or: X_batch, y: Y_batch, prob: dropout_p})\n",
    "\n",
    "            #_, pred = sess.run([train_op, logits], feed_dict = {X_or: X_batch, y: Y_batch, prob: dropout_p})\n",
    "            train_pred.append(pred)\n",
    "            train_truth.append(Y_batch)\n",
    "\n",
    "        for index in range(len(list_sequence_val)):\n",
    "            # batch is size 1 here: stochastic gradient descent\n",
    "            if concat_late:\n",
    "                X_batch_or = np.reshape(list_sequence_val[index][:,:12], (100, num_features))\n",
    "                X_batch_fixed = np.reshape(list_sequence_val[index][0,12:], (1, fixed_num_features))\n",
    "                Y_batch = np.reshape(val_y[index], (1,1))\n",
    "            else:\n",
    "                X_batch = np.reshape(list_sequence_val[index], (100, num_features))\n",
    "                Y_batch = np.reshape(val_y[index], (1,1))\n",
    "                \n",
    "            #X_batch = np.reshape(list_sequence_val[index], (100, num_features))\n",
    "            #Y_batch = np.reshape(val_y[index], (1,1))\n",
    "\n",
    "            pred_avg = 0\n",
    "            for _ in range(num_test_runs):\n",
    "                if concat_late:\n",
    "                    pred = sess.run([logits], feed_dict = {X_or: X_batch_or, X_fixed: X_batch_fixed,\n",
    "                                                           y: Y_batch, prob: 1})\n",
    "                else:\n",
    "                    pred = sess.run([logits], feed_dict = {X_or: X_batch, y: Y_batch, prob: 1})\n",
    "                pred_avg += pred[0]\n",
    "\n",
    "            val_test_pred.append(pred_avg/num_test_runs)\n",
    "            val_test_truth.append(Y_batch)\n",
    "            \n",
    "        if i == 1:\n",
    "            # shuffle because the bags in the list are ranked (increasing order)\n",
    "            range_arr = np.arange(len(val_test_truth))\n",
    "            half_lenght = int(len(val_test_truth)/2)\n",
    "            np.random.shuffle(range_arr)\n",
    "            indices_val, indices_test = range_arr[:half_lenght], range_arr[half_lenght:]\n",
    "        \n",
    "        val_test_truth, val_test_pred = np.array(val_test_truth), np.array(val_test_pred)\n",
    "        val_pred, val_truth = val_test_pred[indices_val], val_test_truth[indices_val]\n",
    "        test_pred, test_truth = val_test_pred[indices_test], val_test_truth[indices_test]\n",
    "\n",
    "        training_loss = compute_rmse(train_pred, train_truth)\n",
    "        val_loss = compute_rmse(val_pred, val_truth)\n",
    "        test_loss = compute_rmse(test_pred, test_truth)\n",
    "    \n",
    "        val_loss_list.append(val_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        \n",
    "        if (i == 1) or (i % 10 == 0):\n",
    "            print(\"EPOCH \" + str(i))\n",
    "            print(\"TRAINING LOSS: \" + str(training_loss))\n",
    "            print(\"VAL LOSS: \" + str(val_loss), \"TEST LOSS: \" + str(test_loss))\n",
    "\n",
    "#     lowest_val_loss = min(val_loss_list)\n",
    "#     loss_test = test_loss_list[val_loss_list.index(lowest_val_loss)]\n",
    "    \n",
    "    min_index = get_min_index(np.array(val_loss_list), 5)\n",
    "    lowest_val_loss = val_loss_list[min_index]\n",
    "    loss_test = test_loss_list[min_index]\n",
    "\n",
    "    return lowest_val_loss, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FOR DIFFERENT NUMBER OF MOMENTS AND THE SAME NUMBER OF PROCESSORS\n",
    "\n",
    "iteration_number = 10\n",
    "random_seed_list = list(range(iteration_number))\n",
    "#number_of_moments = list(range(1,11))\n",
    "number_of_moments = [4]\n",
    "final_val_loss, final_test_loss = [], []\n",
    "\n",
    "for moments in number_of_moments:\n",
    "    print(\"COMPUTING FOR \" + str(moments) + \" MOMENTS\")\n",
    "    validation_loss_list, test_loss_list = [], []\n",
    "    for index in range(iteration_number):\n",
    "        print('COMPUTING FOR ITERATION NUMBER ' + str(index+1))\n",
    "        lowest_val_loss, loss_test_list = inference_attention(random_seed_list[index], 2, 5, True, moments, False)\n",
    "        validation_loss_list.append(np.mean(lowest_val_loss))\n",
    "        test_loss_list.append(np.mean(loss_test_list))\n",
    "        print(np.mean(lowest_val_loss), np.mean(loss_test_list))\n",
    "        \n",
    "    final_val_loss.append(np.mean(validation_loss_list))\n",
    "    final_test_loss.append(np.mean(test_loss_list)) \n",
    "    print(np.mean(validation_loss_list), np.mean(test_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FOR NO MOMENTS WITH DIFFERENT PROCESSORS\n",
    "\n",
    "iteration_number = 10\n",
    "random_seed_list = list(range(iteration_number))\n",
    "processor_steps_list = [2]\n",
    "final_val_loss, final_test_loss = [], []\n",
    "\n",
    "for num_processor_steps in processor_steps_list:\n",
    "    print(\"COMPUTING FOR \" + str(num_processor_steps) + \" PROCESSING STEPS\")\n",
    "    validation_loss_list, test_loss_list = [], []\n",
    "    for index in range(iteration_number):\n",
    "        print('COMPUTING FOR ITERATION NUMBER ' + str(index+1))\n",
    "        lowest_val_loss, loss_test_list = inference_attention(random_seed_list[index], num_processor_steps, 5, False, 1, False)\n",
    "        validation_loss_list.append(np.mean(lowest_val_loss))\n",
    "        test_loss_list.append(np.mean(loss_test_list))\n",
    "        print(np.mean(lowest_val_loss), np.mean(loss_test_list))\n",
    "        \n",
    "    final_val_loss.append(np.mean(validation_loss_list))\n",
    "    final_test_loss.append(np.mean(test_loss_list)) \n",
    "    print(np.mean(validation_loss_list), np.mean(test_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 64)\n",
      "2\n",
      "12\n",
      "64\n",
      "768\n",
      "(65, 256)\n",
      "2\n",
      "65\n",
      "256\n",
      "16640\n",
      "(256,)\n",
      "1\n",
      "256\n",
      "256\n",
      "(128, 64)\n",
      "2\n",
      "128\n",
      "64\n",
      "8192\n",
      "(128, 1)\n",
      "2\n",
      "128\n",
      "1\n",
      "128\n",
      "25984\n"
     ]
    }
   ],
   "source": [
    "# total_parameters = 0\n",
    "# for variable in tf.trainable_variables():\n",
    "#     # shape is an array of tf.Dimension\n",
    "#     shape = variable.get_shape()\n",
    "#     print(shape)\n",
    "#     print(len(shape))\n",
    "#     variable_parameters = 1\n",
    "#     for dim in shape:\n",
    "#         print(dim)\n",
    "#         variable_parameters *= dim.value\n",
    "#     print(variable_parameters)\n",
    "#     total_parameters += variable_parameters\n",
    "# print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Aggregated-MIR (with moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregated_method_moments(n_moments=1):\n",
    "    \n",
    "    random_seed_list = list(range(10))\n",
    "    final_data, labels = moments_df(n_moments)\n",
    "    \n",
    "    final_loss_train, final_loss_test = [], []\n",
    "    count = 1\n",
    "\n",
    "    for index in range(10):    \n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=random_seed_list[index])\n",
    "        list_loss_train, list_loss_test = [], []\n",
    "        for train_index, test_index in kf.split(range(labels.shape[0])):\n",
    "            train_x, test_x = final_data.iloc[train_index,:], final_data.iloc[test_index,:]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_x)\n",
    "            train_x, test_x = scaler.transform(train_x), scaler.transform(test_x)\n",
    "\n",
    "            train_y, test_y = labels.iloc[train_index,:], labels.iloc[test_index,:]\n",
    "\n",
    "            rbf_svr = MLPRegressor(hidden_layer_sizes=(512, ), learning_rate_init=0.001, \n",
    "                                   max_iter=300, alpha=1)\n",
    "            \n",
    "            rbf_svr.fit(train_x, train_y)\n",
    "            train_pred = rbf_svr.predict(train_x)\n",
    "            test_pred = rbf_svr.predict(test_x)\n",
    "\n",
    "            training_loss = np.sqrt(mean_squared_error(np.reshape(np.array(train_pred),(len(train_pred),1)), np.reshape(np.array(train_y), (len(train_pred),1))))\n",
    "            testing_loss = np.sqrt(mean_squared_error(np.reshape(np.array(test_pred),(len(test_pred),1)), np.reshape(np.array(test_y), (len(test_pred),1))))\n",
    "\n",
    "            list_loss_train.append(training_loss)\n",
    "            list_loss_test.append(testing_loss)\n",
    "\n",
    "        final_loss_train.append(np.mean(list_loss_train))\n",
    "        final_loss_test.append(np.mean(list_loss_test))\n",
    "        print('Iteration number ' + str(count) + ' is done')\n",
    "        count += 1\n",
    "\n",
    "    print('The training loss is ' + str(np.mean(final_loss_train)))\n",
    "    print('The test loss is ' + str(np.mean(final_loss_test)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1 is done\n",
      "Iteration number 2 is done\n",
      "Iteration number 3 is done\n",
      "Iteration number 4 is done\n",
      "Iteration number 5 is done\n",
      "Iteration number 6 is done\n",
      "Iteration number 7 is done\n",
      "Iteration number 8 is done\n",
      "Iteration number 9 is done\n",
      "Iteration number 10 is done\n",
      "The training loss is 11.284323043751655\n",
      "The test loss is 35.60138319606699\n"
     ]
    }
   ],
   "source": [
    "aggregated_method_moments(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Instance-MIR (with moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def instance_method(use_moments=False, n_moments=1):\n",
    "\n",
    "    full_data = load_data()\n",
    "    random_seed_list = list(range(10))\n",
    "    \n",
    "    if use_moments:\n",
    "        df, _ = moments_df(n_moments)\n",
    "        col_solar = ['solar_0', 'solar_1', 'solar_2', 'solar_3']\n",
    "        df = df.iloc[np.repeat(np.arange(len(df)), 100)]\n",
    "        df = df[[col for col in df.columns if col not in col_solar]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        full_data = full_data.reset_index(drop=True)\n",
    "        full_data = pd.concat([df, full_data], axis=1)\n",
    "\n",
    "    final_loss_train, final_loss_test = [], []\n",
    "    count = 1\n",
    "    for index in range(10):\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=random_seed_list[index])\n",
    "        cols_exclude = [\"id\", \"label\"]\n",
    "        features = [col for col in list(full_data.columns) if col not in cols_exclude]\n",
    "        list_loss_train, list_loss_test = [], []\n",
    "\n",
    "        for train_index, test_index in kf.split(list(full_data['id'].unique())):\n",
    "            train_index, test_index = np.array(full_data['id'].unique())[list(train_index)], np.array(full_data['id'].unique())[list(test_index)]\n",
    "            train = full_data[full_data['id'].apply(lambda value: value in train_index)]\n",
    "            test = full_data[full_data['id'].apply(lambda value: value in test_index)]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train[features])\n",
    "            train[features], test[features] = scaler.transform(train[features]), scaler.transform(test[features])\n",
    "\n",
    "            train_x, train_y = train[features], train['label']\n",
    "            test_x, test_y = test[features], test['label']\n",
    "\n",
    "            rbf_svr = MLPRegressor(hidden_layer_sizes=(1024,), learning_rate_init=0.001, max_iter=300, alpha=3)\n",
    "            rbf_svr.fit(train_x, train_y)\n",
    "            train_pred = rbf_svr.predict(train_x)\n",
    "            test_pred = rbf_svr.predict(test_x)\n",
    "\n",
    "            df_train_pred = pd.DataFrame(np.concatenate([np.reshape(train_pred, (train_pred.shape[0],1)), \n",
    "                        np.reshape(train['id'].values, (train_pred.shape[0],1))], axis=1))\n",
    "\n",
    "            df_test_pred = pd.DataFrame(np.concatenate([np.reshape(test_pred, (test_pred.shape[0],1)), \n",
    "                        np.reshape(test['id'].values, (test_pred.shape[0],1))], axis=1))\n",
    "\n",
    "            true_train_y, true_test_y = train.groupby(['id']).mean()['label'].values, test.groupby(['id']).mean()['label'].values\n",
    "            train_mean_pred = df_train_pred.groupby(1).mean().values\n",
    "            test_mean_pred = df_test_pred.groupby(1).mean().values\n",
    "\n",
    "            training_loss = np.sqrt(mean_squared_error(np.reshape(train_mean_pred,(train_mean_pred.shape[0],1)), \n",
    "                                                       np.reshape(true_train_y, (train_mean_pred.shape[0],1))))\n",
    "\n",
    "            testing_loss = np.sqrt(mean_squared_error(np.reshape(test_mean_pred,(test_mean_pred.shape[0],1)), \n",
    "                                                      np.reshape(true_test_y, (test_mean_pred.shape[0],1))))\n",
    "\n",
    "            list_loss_train.append(training_loss)\n",
    "            list_loss_test.append(testing_loss)\n",
    "            \n",
    "        final_loss_train.append(np.mean(list_loss_train))\n",
    "        final_loss_test.append(np.mean(list_loss_test))\n",
    "        print('Iteration number ' + str(count) + ' is done')\n",
    "        count += 1\n",
    "\n",
    "    print('The training loss is ' + str(np.mean(final_loss_train)))\n",
    "    print('The test loss is ' + str(np.mean(final_loss_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1 is done\n",
      "Iteration number 2 is done\n",
      "Iteration number 3 is done\n",
      "Iteration number 4 is done\n",
      "Iteration number 5 is done\n",
      "Iteration number 6 is done\n",
      "Iteration number 7 is done\n",
      "Iteration number 8 is done\n",
      "Iteration number 9 is done\n",
      "Iteration number 10 is done\n",
      "The training loss is 12.217802154190945\n",
      "The test loss is 24.602675029887493\n"
     ]
    }
   ],
   "source": [
    "instance_method(False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
